---
title: "Class8"
author: "Aisha Mohamed (PID A16297530)"
format: pdf
---

## Data Input 

The data is supplied on CSV format. 

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

Now I will store the diagnosis column for later and exclude from the data set I will actually 
do things with. 


```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

> Q1. How many people are in this data set?

```{r}
nrow(wisc.df)
```

> Q2. How many of the observations gave a malignant disease?

```{r}
table(wisc.df$diagnosis)
```


```{r}
sum(wisc.df$diagnosis == "M")
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
x <- colnames(wisc.df)
suffix <- grep("_mean$", x, value = T)
length(suffix)
```


## Principal Component Analysis

We need to scale our data before PCA as some of the columns are measured in terms of different
units with different means and different variance. The upshot here is we set `scale = TRUE`
argument to `prcomp()`


```{r}
wisc.pr <- prcomp(wisc.data, scale = T)

summary(wisc.pr)

```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis)
```



> Q4. From your results, what proportion of the original variance is captured by the 
first principal components (PC1)?

```{r}
summary(wisc.pr)$importance[2,1]
```

> Q5. How many principal components (PCs) are required to describe at least 70% of the 
original variance in the data?

```{r}
summary(wisc.pr)$importance[2,1:3]
length(summary(wisc.pr)$importance[2,1:3])

#There are 3 PCs that are required to reach at least 70%. 

```


> Q6. How many principal components (PCs) are required to describe at least 90% of the 
original variance in the data?

```{r}
summary(wisc.pr)$importance[2,1:7]
length(summary(wisc.pr)$importance[2,1:7])

#There are 7 PCs that are required to reach at least 90%. 
```



Create biplot of wisc.pr

```{r}
biplot(wisc.pr)
```


> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot seems too cluttered and has a lack of organization. Very difficult to interpret. 



Scatter plot observations by components 1 and 2. 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about 
these plots?

Repeat scatter plots for Component 1 and 3. 

```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```

Due to PC2 having more variance than PC3, the PC1 vs PC2 plot separates the malignant and 
benign groups better than the latter plot. 



Use ggplot to display the plots a bit differently. 


```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + aes(PC1, PC2, col = diagnosis) +  geom_point()
```



Produce Scree Plots.


```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```



```{r}
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```




> Q9. For the first principal component, what is the component of the loading vector 
(i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```



```{r}
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```


> Q10. What is the minimum number of principal components required to explain 80% of 
the variance of the data?

```{r}
summary(wisc.pr)$importance[2,1:5]
length(summary(wisc.pr)$importance[2,1:5])
```




# Hierarchical Clustering 


```{r}
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method = "complete")


```


> Q11. Using the plot() and abline() functions, what is the height at which the clustering 
model has 4 clusters?


```{r}
plot(wisc.hclust)

abline(h = 19, col = "red", lty = 2)
```

Selecting number of clusters. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

table(wisc.hclust.clusters, diagnosis)

```


> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number
of clusters between 2 and 10?

```{r}
for (i in c(2:10)) {
  wisc.hclust.clusters <- cutree(wisc.hclust, k=i)
  table(wisc.hclust.clusters, diagnosis)
}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```


> Q13. Which method gives your favorite results for the same data.dist dataset? Explain 
your reasoning.

I think I liked using the `abline()` function as you can see how theline crosses the
clusters, giving a clear cut distinction. 


##5 Combinding methods (up to Q15)


```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object. 

```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```


```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = grps, 
     xlab = "PC1", ylab = "PC2")
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```



```{r}
g <- as.factor(grps)
levels(g)
```


```{r}
g <- relevel(g,2)
levels(g)
```


```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g, xlab = "PC1", ylab = "PC2")
```



> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")

# Cut into 2 clusters
wisc.pr.hclust.clusters<-cutree(wisc.pr.hclust, k=2)

# Compare 
table(wisc.pr.hclust.clusters, diagnosis)
```



## Sensitivity/Specificity

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

I would say best specificity is hclust. Also although it was optional, kmeans has to be the 
best sensitivity from what I've read up on it. 


## Prediction 

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```


> Q18. Which of these new patients should we prioritize for follow up based on your results?

Out of the two, Patient 2 should have the most priority for a follow up due to their 
association with a malignant disease. 